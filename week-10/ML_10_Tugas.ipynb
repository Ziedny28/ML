{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCZgQkH7q0L62p59+lAWoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ziedny28/ML/blob/master/week-10/ML_10_Tugas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "R1w2S7Nolz1_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TPNgEDXlCM_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjJXtGXWlG9m",
        "outputId": "d9f7282e-a86c-4d95-a533-efac53e2ffaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE52QVmglRTz",
        "outputId": "4791e252-7d66-4ff2-9412-fc38e022abe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZolxwEglZJF",
        "outputId": "e488fa1f-c028-47d0-df23-e330cdd8b41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ6L1NGfla9z",
        "outputId": "9e51558a-075b-4997-ee8d-62c88f324b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Olah Teks"
      ],
      "metadata": {
        "id": "JHpwDsdMn7_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJrhkZavlemO",
        "outputId": "09c0a53d-d75d-41e8-d9d0-46c9c4bab20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "3rQszHxGlhgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njrBu9PblnVM",
        "outputId": "50f17e6e-e9db-4c61-83d2-e7a3fc65510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "42ffLQYbltYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOjmXlXdoBTT",
        "outputId": "dbcc2d01-bd98-446e-87db-990615d4fd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJZdQArhoDZn",
        "outputId": "9a0c7003-c956-4284-dc27-3f1815eb279b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "StCDkwAboHOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediksi"
      ],
      "metadata": {
        "id": "SC5iQ6fPoPqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat Trianing Set dan Target"
      ],
      "metadata": {
        "id": "fgHjDhvQoR5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYu0iVxpoMfY",
        "outputId": "cc7b8426-203a-4c76-a6b0-a585123475d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "sEyKHJpNoUFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG5ye3mvoWzK",
        "outputId": "997e935a-fbea-43d4-8525-f0c0e15e145a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=100"
      ],
      "metadata": {
        "id": "y8MGnYkDoZNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPo_SQuKogsx",
        "outputId": "76ec530d-e5a6-4e98-aebd-2afa51d1417c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiRbVLhGoj8x",
        "outputId": "904b5a76-f961-44f2-c4ce-f81c29a1d21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "9Rf8gR0Volcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zen_OfBAopmx",
        "outputId": "0dd4d333-0aa6-4160-f632-8e33137f44b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "nG2WbeCLossg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwKIagTtot4d",
        "outputId": "4429382d-7063-42c5-cf44-52f4d1319337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat Batch Training"
      ],
      "metadata": {
        "id": "DMrHQea0o5h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-LT2fYgoyF3",
        "outputId": "4e348021-785e-43af-f610-285827a609b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buat Model"
      ],
      "metadata": {
        "id": "y2wu2Dpro8S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "FOC2cr0ao2kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "52JBihe1o99T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "dDCr7WKfo_RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uji Model"
      ],
      "metadata": {
        "id": "vc4ogjZJpFZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Bj9w1fpAhB",
        "outputId": "83b30712-ec70-4938-aa33-d74c7b287375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clwAAItApG7j",
        "outputId": "2e8dcb8a-9fa0-4978-cc39-509906010588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "vq_XCX1hpIOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUBwx2xYpLOO",
        "outputId": "3c29b0a6-0c1e-4960-ea3f-02dd06122958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([53, 57, 50, 64,  3,  2, 25, 61, 35, 42,  7, 18, 54, 30,  8,  3, 19,\n",
              "       24,  6, 14, 53, 20,  3, 55,  8, 50, 56, 25, 51, 22, 65,  4, 44, 54,\n",
              "       34, 43, 14, 18, 54, 44, 50, 31,  8, 27, 45, 54, 62, 65, 22, 51, 45,\n",
              "        9, 65,  9,  8, 15, 44, 40, 26, 51, 39, 46, 23,  2, 45, 53, 53, 32,\n",
              "       28, 45, 10, 27, 21,  3, 28, 28, 27, 27, 44, 31,  5, 52, 59, 16, 60,\n",
              "       11, 64, 15, 39, 16, 47, 13, 58,  6,  1, 62,  9, 14, 19, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py1Y0HbcpNLk",
        "outputId": "b96fcde8-edff-42f4-b9bc-4fbf2b3fb045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'\\nThat we have had no time to move our daughter:\\nLook you, she loved her kinsman Tybalt dearly,\\nAnd s'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"nrky! LvVc,EoQ-!FK'AnG!p-kqLlIz$eoUdAEoekR-NfowzIlf.z.-BeaMlZgJ fnnSOf3NH!OONNeR&mtCu:yBZCh?s'\\nw.AFH\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "3f_S10cmpUb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "kE_EsFTvpOCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA7a0kgYpXC1",
        "outputId": "a79d3a30-a6fa-47c1-e98a-f62abdf2ef86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1885586, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkMUqaOcpZo0",
        "outputId": "1825c93d-517c-4058-8ba5-0edfce06381e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.92769"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "hG4fhMWVpcnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konfigurasi Checkpoint\n"
      ],
      "metadata": {
        "id": "d2Y9zYNFpgvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "ErghaZ0PpdiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lakukan Proses Training"
      ],
      "metadata": {
        "id": "m7RhaLMWpkzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "_q7pzHOMpi6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZouq_v6pmV5",
        "outputId": "501672cf-d351-4168-b763-9d01eb0fa02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 20s 58ms/step - loss: 2.7337\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 2.0017\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.7278\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.5633\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.4624\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3923\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3386\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2930\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2520\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2138\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1739\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1331\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0916\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0472\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.9991\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.9497\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.8976\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 0.8447\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7933\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Text"
      ],
      "metadata": {
        "id": "vbaHScX7ptN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "nx5-VUVpppbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "zMpih7XSpzXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecMiRDZzp2j1",
        "outputId": "ed6a98d3-fb22-458f-8955-f67e3b085772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I naver had a taste or a people.\n",
            "\n",
            "Clown:\n",
            "Alack the torceiff, yet we will answer thee\n",
            "As thou lovest me not.\n",
            "\n",
            "SICINIUS:\n",
            "'Tis poorly.\n",
            "\n",
            "SOMEY:\n",
            "Soid thou that torn me come on; and did he learn,\n",
            "Whom I enjoy, you are too sword more violenter.\n",
            "I am Grumioved, his pager and rab's supper,\n",
            "To entreat her grave bears hither comes for him.\n",
            "Where is no more adoptiness?\n",
            "They are but offer theats than he, and seem to\n",
            "brief a happy thing.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sir, your most noble sir,\n",
            "Does hot the king did carry to do.\n",
            "3, he were let you better your infume, cried,\n",
            "Like a trueble me mad, but in the shout out for,\n",
            "An end between a wit of his toom.\n",
            "\n",
            "LEONTES:\n",
            "Traitor,\n",
            "you must be, sir, charity you of my consent?\n",
            "\n",
            "Shepherd:\n",
            "I will go care it? I said 'starn'd thee for\n",
            "the shepherds: for I will hence again; but I think\n",
            "I'll keep my books, he's the villain yet.\n",
            "\n",
            "TRANIO:\n",
            "If it be sound the time?\n",
            "\n",
            "BIONDELLO:\n",
            "Sir, a dell!\n",
            "God for she is the comfortable women\n",
            "Tranio in your honour's mother;\n",
            "With and not wise men: b \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2621195316314697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipmnC0_pp6UB",
        "outputId": "dd3974cf-cb4b-4d7d-df8c-8852f9f5d4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\n\\nHORTENSIO:\\nSir, you must not like to open thee.\\n\\nCAPULET:\\nConfess the truth, man; advise him evenly.\\n\\nQUEEN MARGARET:\\nSluck him hence, both jade-mouther all's to me.\\nFear, good boy, he will answer thy love,--\\n\\nSICINIUS:\\nThe good idon those vault the face.\\n\\nPAULINA:\\nGre me the first, abide before, and have procked in Padua\\nOf my guilty hunt the time.\\n\\nPage:\\nHis name is Like,' and 'Tway with force my back and blows.\\nWhen hath yesbraced and pretence of this action\\nTransport him outwoman: or when he is\\nPartled, hence I have speak of thee;\\nHow fares our children norship modesty?\\nLife the news had I, that once thou darest end\\nHer auder bleeds at heaven and her princels.\\nMyself am steps ho, hell, and have her kind,\\nso proud as you the devil.\\n\\nPONfIE:\\nWhat, wearly is the strength of our misery,\\nis rise and fear'd to appear what means this true\\nLord Hastings that mercy in him\\nAs I could seem to enter. Your comband\\nHow do I throw doing these mighty lunge?\\n\\nFirst Murderer:\\nOne on my father cann\"\n",
            " b\"ROMEO:\\nAnd if I were past our naked, is the palace I have still,\\nAnd so he cries 'Denio; come you the ground,\\nWere perceive you from some spring to the court.\\n\\nEXETER:\\nFirst, look to thee again,\\nOur love, your flesh and bloody king!\\nBut what art thou that, give me patience?\\n\\nQUEEN MARGARET:\\nAnd that Lucentio, and I care not.\\n\\nPETER:\\nI saw her health! and, but spices! farewell.\\n\\nFRIAR LAURENCE:\\nRomeo.\\n\\nFRANCIO:\\nFathering, for the castle cases! O' on your arg, low asleep.\\n\\nFirst Lord:\\nIt must move thee,\\nBark, alike most caused against your head.\\nNo, by his native I shall stand upon it.\\n\\nFarthind Washion' to age?\\n\\nProvost:\\nI would have made me gainfant still it not.\\n\\nTRANIO:\\nAy, forsooth, I think,\\nMy pity hate we meet thee. For I take my sweet heart\\nTo you the king i' the maid:\\nAnd you the hour their friends as fas\\nto-day. When at the herd sensest bleems, his head,\\nAnd since it perceives in the duke's,\\nAnd bid them burstray; to have lovidged her here and half-headed\\nThrives the first that words\"\n",
            " b\"ROMEO:\\nO, no! the next differences,\\nLet him pity of the time to come.\\n\\nHENRY BOLINGBROKE:\\nI have free's tribune, to acquaint her he may live:\\nNot lose the swallow marber wrongs:\\nSouching nature but the Lady Brannon,\\nTo batch by herself, as if he drawn\\nShould die their waves seem'd. Catesby!\\n\\nGRUMIO:\\nWhy, there's some water mayst thou uncles?\\nAh, what is that, to term the rest.\\n\\nBRUTUS:\\nGood Montfoly; here's a change hence to France,\\nAnd not be in himself, but with a day.\\nBut strange, mouth with injustice straight;\\nWith that word 'banished,' comes counterpoiss.\\n\\nCALIENCE:\\nO myself is banish'd from this agting tale,\\nby his heart is counteam'd, and she cannot hold\\nIn sadry, and come have tongued deliver\\nBeneverity to London you,\\nBy victory yielding thence.\\n\\nQUEEN:\\nSir, if you'ld satisfy me in what end about me?\\nShow Edward's friends by the king, who,\\nit is made eye behond ourselves the wills of Lancaster.\\nDid not money? let him be sin, out o' the strange of a\\nman!--Nay'r here. And 'twixt your b\"\n",
            " b\"ROMEO:\\nGood Paunin, say Ahe not the Aun,\\nThat so bring in a barthen will access to wear.\\nOne, well, but 'tis an ill old men\\nwe are mercial as the king.\\n\\nHENRY BOLINGBROKE:\\nO harm bore a husband;\\nMy business is as the Glouchsafe well.\\nNo, for my grandfore--to be mock'd for Tains,\\nTell me what you would live have laid it with an elve.\\n\\nPAULINA:\\nI am liberty.\\n\\nGLOUCESTER:\\nAs I'll vill them thy master's man.\\n\\nDUKE OF YORK:\\n\\nYORK:\\nIf there shall he be parted to know forth.\\n\\nBUCKINGHAM:\\nI have a husband, be not so: althout as here\\nDuck with God, in good time, how scraggaret spends you;\\nFor showing be with either hand.\\nI'll keep you come to your kind puts,\\nFor, then in Lord steep'd like chances!\\nLet me distilly acquainted with thy lips.\\n\\nCATESBY:\\nMy lord,\\nDeath, my mind is cheap,--\\nBent of Peter God the trumpets, savine\\nAnd they have long heaven and yet about down.\\n\\nKING RICHARD III:\\nA flesh tyrant! move not yine, fairly letth him go\\nWe know the crown that I do call them went,\\nThinks he so little o\"\n",
            " b\"ROMEO:\\nWhat, art thou yourself are traitor?\\n\\nMERCUTIO:\\nThe strangen is out at geasons born.\\n\\nEDWARD:\\nSupport, so I dream or something overnice\\nThan they that will be load to combat up:\\nIn that are thy great self-will be\\nhis name and clear in abroad in air\\nOr in a stark, will be urgined.\\n\\nLUCIO:\\nA hundred, if you writ, I have alters the wills be.\\nMe that e'er was hit to hear it; you shall have\\na bode torment my young podder young Marcius\\nWere beated timul, Edmand a husband,\\nTo leave up incertain that here all the wares on himself more\\nThan they that gives the Warped so lour with begh\\nAnd never mortal eyes to report.\\n\\nGLOUCESTER:\\nI cannot hold you to his realm;\\nThe ching from this one dangerous plaints and entrance\\non thee the morning, all the walls, will heart\\nMore than in three and thy new grown tongue.\\n\\nARCHIDAMUS:\\nI will not marry equal in my touch:\\nI never heard are comes well fills you like a grave\\nWith all the waters of her tears?\\n\\nARIEL:\\nAlas! please your thief, of antignny,\\nor a hair \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.34360671043396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekspor model Generator"
      ],
      "metadata": {
        "id": "XlYYLmzGp_iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPxjT9Gsp7lJ",
        "outputId": "fa24c47b-8250-458b-ebe3-002e412f43ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7d5db316b130>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYfV93sRqBrY",
        "outputId": "26afa020-0f49-4b7e-9414-b92e1e1f692d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Sir, yourself, you'll know not one of you:\n",
            "Then where you will, even thus;\n",
            "Thou shalt be a'lly warr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas"
      ],
      "metadata": {
        "id": "euX4o1G5RD9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-HDshizaRD6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "aUfa4-siqCvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "xZg1KSh8Rj-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "pS9kNNQKR1LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evgPjqlgR47i",
        "outputId": "9e93687b-c873-48d3-cca4-09ed5936f4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 59ms/step - loss: 2.7124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d5db1bbae00>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9LwxQHbR6Un",
        "outputId": "3d1a494d-5575-4875-ccbc-6311d433a89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1535\n",
            "Epoch 1 Batch 50 Loss 2.0429\n",
            "Epoch 1 Batch 100 Loss 1.9287\n",
            "Epoch 1 Batch 150 Loss 1.8448\n",
            "\n",
            "Epoch 1 Loss: 1.9888\n",
            "Time taken for 1 epoch 16.99 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8394\n",
            "Epoch 2 Batch 50 Loss 1.7604\n",
            "Epoch 2 Batch 100 Loss 1.6573\n",
            "Epoch 2 Batch 150 Loss 1.6780\n",
            "\n",
            "Epoch 2 Loss: 1.7131\n",
            "Time taken for 1 epoch 11.44 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5804\n",
            "Epoch 3 Batch 50 Loss 1.6111\n",
            "Epoch 3 Batch 100 Loss 1.5473\n",
            "Epoch 3 Batch 150 Loss 1.5573\n",
            "\n",
            "Epoch 3 Loss: 1.5547\n",
            "Time taken for 1 epoch 11.64 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4642\n",
            "Epoch 4 Batch 50 Loss 1.4695\n",
            "Epoch 4 Batch 100 Loss 1.4414\n",
            "Epoch 4 Batch 150 Loss 1.4954\n",
            "\n",
            "Epoch 4 Loss: 1.4557\n",
            "Time taken for 1 epoch 11.98 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3566\n",
            "Epoch 5 Batch 50 Loss 1.3546\n",
            "Epoch 5 Batch 100 Loss 1.3772\n",
            "Epoch 5 Batch 150 Loss 1.3874\n",
            "\n",
            "Epoch 5 Loss: 1.3865\n",
            "Time taken for 1 epoch 12.13 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3186\n",
            "Epoch 6 Batch 50 Loss 1.3591\n",
            "Epoch 6 Batch 100 Loss 1.3372\n",
            "Epoch 6 Batch 150 Loss 1.3197\n",
            "\n",
            "Epoch 6 Loss: 1.3338\n",
            "Time taken for 1 epoch 11.78 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.3000\n",
            "Epoch 7 Batch 50 Loss 1.2814\n",
            "Epoch 7 Batch 100 Loss 1.3015\n",
            "Epoch 7 Batch 150 Loss 1.3097\n",
            "\n",
            "Epoch 7 Loss: 1.2877\n",
            "Time taken for 1 epoch 11.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2489\n",
            "Epoch 8 Batch 50 Loss 1.2463\n",
            "Epoch 8 Batch 100 Loss 1.2545\n",
            "Epoch 8 Batch 150 Loss 1.2264\n",
            "\n",
            "Epoch 8 Loss: 1.2468\n",
            "Time taken for 1 epoch 10.87 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1808\n",
            "Epoch 9 Batch 50 Loss 1.1970\n",
            "Epoch 9 Batch 100 Loss 1.2128\n",
            "Epoch 9 Batch 150 Loss 1.1739\n",
            "\n",
            "Epoch 9 Loss: 1.2068\n",
            "Time taken for 1 epoch 11.60 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1581\n",
            "Epoch 10 Batch 50 Loss 1.1565\n",
            "Epoch 10 Batch 100 Loss 1.1488\n",
            "Epoch 10 Batch 150 Loss 1.1633\n",
            "\n",
            "Epoch 10 Loss: 1.1675\n",
            "Time taken for 1 epoch 10.98 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perbedaan"
      ],
      "metadata": {
        "id": "GPmrcPIFZVjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perbedaan utama antara tugas dan praktikum 2 terletak pada train yang digunakan. Praktikum 2 menggunakan train yang lebih umum dan sederhana dengan model.fit, sedangkan kode tuga menggunakan train yang khusus dan kompleks. Dalam pendekatan ini, ada penggunaan metode train_step dalam model turunan yang mengatur pelatihan pada tingkat batch. Hal ini termasuk perhitungan loss, gradien, pembaruan bobot model, serta penggunaan tf.metrics.Mean untuk menghitung rata-rata loss selama pelatihan. Pendekatan ini memberikan lebih banyak kontrol dan fleksibilitas dalam pengaturan pelatihan model."
      ],
      "metadata": {
        "id": "V9HPfvm07OcT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATyuEY6M7N0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}